{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "if your dataset is very large then you can split it into several TFRecords files called shards. This will also improve the random shuffling, because the Dataset API only shuffles from a smaller buffer of e.g. 1024 elements loaded into RAM. So if you have e.g. 100 TFRecords files, then the randomization will be much better than for a single TFRecords file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jojinwoo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 subjects: [211 212 213 216 220 224 225 226 230 237 240 252 259 262 276 281 284 401\n",
      " 404 408 411 413 417 427 437 438 439 443 446 449 450 452 471 472 476 482\n",
      " 484 485]\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/mimicdb/180521_ch2_30s/';\n",
    "filename = path + 'subject.txt';\n",
    "subject_list = np.loadtxt(filename, delimiter='\\t', dtype=np.int16)\n",
    "subjects = np.unique(subject_list[:,1])\n",
    "print(len(subjects), \"subjects:\", subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(path, subject_id):\n",
    "    f = h5py.File(path + 'subject' + subject_id + '.mat')\n",
    "    s = f['/subject/systolic'].value.T.astype(int)\n",
    "    d = f['/subject/diastolic'].value.T.astype(int)\n",
    "    e = f['/subject/ecg'].value.T\n",
    "    p = f['/subject/ppg'].value.T\n",
    "    i = f['/subject/index'].value.T.astype(int)\n",
    "    return s, d, e, p, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(count, total):\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count) / total\n",
    "\n",
    "    # Status-message.\n",
    "    # Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_int64(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_float(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading and writing data along with the class-labels to a TFRecords file. This loads and decodes the images to numpy-arrays and then stores the raw bytes in the TFRecords file. If the original image-files are compressed e.g. as jpeg-files, then the TFRecords file may be many times larger than the original image-files.\n",
    "\n",
    "It is also possible to save the compressed image files directly in the TFRecords file because it can hold any raw bytes. We would then have to decode the compressed images when the TFRecords file is being read later in the `parse()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(ecg, ppg, systolic, diastolic, indices, out_path):\n",
    "    print(\"\\nConverting: \" + out_path)\n",
    "\n",
    "    writer = None\n",
    "\n",
    "    # Number of signals. Used when printing the progress.\n",
    "    num_signals = len(systolic)\n",
    "    \n",
    "    for i, (e, p, s, d, a) in enumerate(zip(ecg, ppg, systolic, diastolic, indices)):\n",
    "        if (i % 64 == 0):\n",
    "            if writer:\n",
    "                writer.close()\n",
    "            writer = tf.python_io.TFRecordWriter(out_path + \"-\" + str(i) + \".tfrecords\")\n",
    "        \n",
    "        # Print the percentage-progress.\n",
    "        print_progress(count=i, total=num_signals-1)\n",
    "\n",
    "        # Create a dict with the data we want to save in the\n",
    "        # TFRecords file. You can add more relevant data here.\n",
    "        data = \\\n",
    "            {\n",
    "                'ecg': wrap_float(e),\n",
    "                'ppg': wrap_float(p),\n",
    "                'systolic': wrap_int64(s),\n",
    "                'diastolic': wrap_int64(d),\n",
    "                'annotation' : wrap_int64(a)\n",
    "            }\n",
    "        \n",
    "        # Wrap the data as TensorFlow Features.\n",
    "        feature = tf.train.Features(feature=data)\n",
    "        \n",
    "        # Wrap again as a TensorFlow Example.\n",
    "        example = tf.train.Example(features=feature)\n",
    "        \n",
    "        # Serialize the data.\n",
    "        serialized = example.SerializeToString()\n",
    "        \n",
    "        # Write the serialized data to the TFRecords file.\n",
    "        writer.write(serialized)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s211\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s212\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s213\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s216\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s220\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s224\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s225\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s226\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s230\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s237\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s240\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s252\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s259\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s262\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s276\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s281\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s284\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s401\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s404\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s408\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s411\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s413\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s417\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s427\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s437\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s438\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s439\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s443\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s446\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s449\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s450\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s452\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s471\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s472\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s476\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s482\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s484\n",
      "- Progress: 100.0%\n",
      "Converting: D:/mimicdb/180521_ch2_30s/tfrecords-batches/s485\n",
      "- Progress: 100.0%"
     ]
    }
   ],
   "source": [
    "for subject in subjects:\n",
    "    sid = \"%03d\" % (subject)\n",
    "    s, d, e, p, i = loadmat(path, sid)\n",
    "    record_tfrecords = os.path.join(path, \"tfrecords-batches/s\" + str(sid))    \n",
    "    convert(ecg=e, ppg=p, systolic=s, diastolic=d, indices=i, out_path=record_tfrecords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
