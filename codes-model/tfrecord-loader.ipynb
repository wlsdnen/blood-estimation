{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "if your dataset is very large then you can split it into several TFRecords files called shards. This will also improve the random shuffling, because the Dataset API only shuffles from a smaller buffer of e.g. 1024 elements loaded into RAM. So if you have e.g. 100 TFRecords files, then the randomization will be much better than for a single TFRecords file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jojinwoo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39  55 211 212 213 216 220 224 225 226 230 237 240 248 252 259 262 276\n",
      " 281 284 401 404 408 410 411 413 417 427 430 437 438 439 443 446 449 450\n",
      " 452 456 466 471 472 474 476 477 480 482 484 485]\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/mimicdb/180331_10/';\n",
    "filename = path + 'filelist.csv';\n",
    "subjects = np.loadtxt(filename, delimiter=',', dtype=np.int16)\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/mimicdb/180331_10/train.tfrecords'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tfrecords_train = os.path.join(path, \"train.tfrecords\")\n",
    "path_tfrecords_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/mimicdb/180331_10/test.tfrecords'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tfrecords_test = os.path.join(path, \"test.tfrecords\")\n",
    "path_tfrecords_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized):\n",
    "    # Define a dict with the data-names and types we expect to find in the TFRecords file.\n",
    "    features = \\\n",
    "        {\n",
    "            'ecg'  : tf.FixedLenFeature([2000,], tf.float32),\n",
    "            'ppg'  : tf.FixedLenFeature([2000,], tf.float32),\n",
    "            'label': tf.FixedLenFeature([1,], tf.int64),\n",
    "            'index': tf.FixedLenFeature([8,], tf.int64)\n",
    "        }\n",
    "    \n",
    "    # Parse the serialized data so we get a dict with our data.\n",
    "    parsed = tf.parse_single_example(serialized=serialized, features=features)  \n",
    "    \n",
    "    return parsed['ecg'], parsed['ppg'], parsed['label'], parsed['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def input_fn(filenames, train, batch_size=32, buffer_size=2048):\n",
    "    # Args:\n",
    "    # filenames:   Filenames for the TFRecords files.\n",
    "    # train:       Boolean whether training (True) or testing (False).\n",
    "    # batch_size:  Return batches of this size.\n",
    "    # buffer_size: Read buffers of this size. The random shuffling is done on the buffer, so it must be big enough.\n",
    "\n",
    "    # Create a TensorFlow Dataset-object which has functionality for reading and shuffling data from TFRecords files.\n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames)\n",
    "\n",
    "    # Parse the serialized data in the TFRecords files.\n",
    "    # This returns TensorFlow tensors.\n",
    "    \n",
    "    dataset = dataset.map(parse)\n",
    "    \n",
    "    if train:\n",
    "        # If training then read a buffer of the given size and\n",
    "        # randomly shuffle it.\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "        # Allow infinite reading of the data.\n",
    "        num_repeat = None\n",
    "    else:\n",
    "        # If testing then don't shuffle the data.\n",
    "        # Only go through the data once.\n",
    "        num_repeat = 1\n",
    "\n",
    "    # Repeat the dataset the given number of times.\n",
    "    dataset = dataset.repeat(num_repeat)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Create an iterator for the dataset and the above modifications.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    # Get the next batch of images and labels.\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=path_tfrecords_train, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    return input_fn(filenames=path_tfrecords_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feature1, feature2, labels, indices = sess.run(train_input_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(feature1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
